# -*- coding: utf-8 -*-
"""PHYS243 Midterm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k_u_pz0ZeloGeFF5ujZ6CkRBNfwFhRB7

PHYS243 Summer Midterm

1. Understanding and Explaining the Dataset

The wine quality dataset is a single-column CSV file with the variable labels listed in the first row. The measurements of biochemical ratings of the wine are joined together in a single column with each row, with the exception of the first, representing one wine evaluation. Since the data contains column labels, it is an example of a supervised learning algorithm, so the intent of this project is to determine conditional probabilities of the measurements given previously disclosed/labeled data. 

The number of instances are as follows: 1599 red wines, 4898 white wines.There are twelve attributes, as follows: fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates, alcohol, quality (score between 0 and 10). The data "label" (which is the datapoint that will be predicted by the model) is the overall quality score assigned to each wine based off of the measurements of the remaining eleven attributes. Therefore, the data labels are single numbers ranging from 0 to 10.

The data source is as follows: 
P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by      data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553. ISSN: 0167-9236.

The intention of the exercise is to format and clean the data in order to divide it into training and test sets. The test set will be delineated and then stored without viewing to maintain integrity of validation results. This step is taken so that over-fitting can be avoided. If all points are give equal weight in the model or too many points are added to the model considering the number of wines listed, the model will not give accurate predictions since due to excessive "background noise" from the taking too many measurements into account.
"""

import scipy.cluster.hierarchy
import numpy as np
import random
import matplotlib.pyplot as plt
import csv
import pandas as pd

plt.rc('text', usetex=True)
plt.rc('font', family='serif')

#from google.colab import files
#uploaded = files.upload()

from google.colab import drive
drive.mount('/content/gdrive')

# Importing the data as a pd.dataframe

import pandas as pd

df=pd.read_csv('/content/gdrive/My Drive/Wine (1).csv', sep = ";", header = None)

#parse header and pass as column. define separation to be ";". for loop for every character. Make a list
# see string ".split(use ; as separator). pass columns argument inside pd.read

df2 = pd.DataFrame(df)
df2.columns = ["fixed acidity", "volatile acid", "citric acid","residual sugar","chlorides","free sulfur dioxide","total sulfur dioxide","density","pH","sulphates", "alcohol", "quality"]

df2.head(3) # Testing data structure

# Make rankings categorical according to my assumptions of what the subjective rankings may indicate.

import seaborn as sns
# %matplotlib inline

quality = df2["quality"].values
category = []
for num in quality:
    if num<4:
        category.append("Bad")
    elif num>7:
        category.append("Good")
    else:
        category.append("Mid")
        
category = pd.DataFrame(data=category, columns=["category"])
data = pd.concat([df2,category],axis=1)

data.head(3) # Testing data structure after manipulations. It is correct.

# Making a plot of correlations to see which variables may impact the model the most.
# This is similar to cor.plot in R. Not in lectures, but a helpful visual.

plt.figure(figsize=(12,6))
sns.heatmap(df.corr(),annot=True)

"""From the plot of correlation values, the relationships between variables can be evaluated to help determine the most highly "associated" values. A correlation value of at least 0.65 is worth exploring, as an assumption. The most promising relationships are as follows:
1) citric acid and fixed acidity
2) density and fixed acidity
3) pH and fixed acidity
4) total sulfur dioxide and free sulfur dioxide


A few of these relationships, besides density and fixed acidity, intuitively seem like they may have confounding factors in their relationships due to the properties of acidity. These factors will be ignored for the sake of simplicity during this analysis.
"""

from sklearn.preprocessing import LabelEncoder
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report,accuracy_score

"""Training and Testing Data Delineation"""

X= data.iloc[:,:-1].values
y=data.iloc[:,-1].values

labelencoder_y =LabelEncoder()
y= labelencoder_y.fit_transform(y)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,random_state=0)

knn = KNeighborsClassifier()
knn.fit(X_train,y_train)
pred_knn=knn.predict(X_test)
print(classification_report(y_test, pred_knn))

"""In conclusion, this KNN model has an accuracy ratio of 0.89, which is fairly representative of the data behavior when tested against the test dataset.

As a final note, I used the website listed below as a reference although the dataset was different and code practices had to be edited by myself to reflect my strategy, opinion, and selection of important variables. Additionally, the algorithm codes were adapted from those provided in the class notebooks.

https://www.kaggle.com/muammerhuseyinoglu/prediction-of-wine-quality
"""

The remaining cells of code are taken from the lectures and my own manipulation and were used along the way in understanding the data but not in the final analysis.
I am keeping them in this repository for future reference but not as part of the midterm's "finished product".

# Copying over for reference (Week 5)

class Point:
    def __init__(self, features, label = None, \
                 name = "Unassigned"):
        self.name = name
        self.features = features
        self.label = label

    # get the dimensionality of the features
    def get_dimension(self):
        return len(self.features)

    def get_features(self):
        return self.features

    def get_label(self):
        return self.label

    def get_name(self):
        return self.name

    def distance_Minkowski(self, other, p = 2):
        return Minkowski_d(self.features, other.get_features(), p)
    distance_Minkowski.__doc__ = Minkowski_d.__doc__

    def get_norm(self, p = 2):
        _zero_=[0 for _ in range(self.get_dimension())]
        return Minkowski_d(self.features, _zero_, p)

    def __str__(self):
        return self.name +" : "+ str(self.features) + ' : '\
               + str(self.label)
      
    ###################### Clustering ############################
    
    class Cluster:
      def __init__(self, points):
        self.points = points
        self.centroid = self.find_centroid()

    def find_centroid(self):
        _sum_ = np.zeros(self.points[0].get_dimension())
        for point in self.points:
            _sum_+=np.array(point.get_features())
        Centroid_vec = _sum_/len(self.points)
        centroid = Point(Centroid_vec, name = "Centroid")
        return centroid

    def update(self, points):
        # Keep the old centroid
        previous_centroid = self.centroid
        # Update the Cluster attribiutes
        self.points = points
        self.centroid = self.find_centroid()
        return self.centroid.distance_Minkowski(previous_centroid)

    def variability(self, p = 2):
        _sum_distances_=0
        for point in self.points:
            _sum_distances_ += point.distance_Minkowski(self.centroid, p)
        return _sum_distances_


    def Elements(self):
        for point in self.points:
            yield point

    def __str__(self):
        names = []
        for point in self.points:
            names.append(point.get_name())
        names.sort()
        info = "Cluster Centroid: " \
               + str(self.centroid.features) +  "contains:" + "\n"
        for name in names:
            info = info + name + ", "
        return info[:-2] #remove trailing comma and space

# Copied from week 5 for reference (Dissimilarity)

def Dissimilarity(_clusters_, p = 2):
    _tot_vari = 0
    for _cluster_ in _clusters_:
        _tot_vari += _cluster_.variability(p)
    return _tot_vari

# Importing Wine dataset
#import csv



features_by_index = {}
_data = {}
  
with open('/content/gdrive/My Drive/Wine.csv', "r") as f:
      lines = f.readlines()
       # create list of field column labels
      header = lines[0].replace('"', '').replace("\n", '').split(";")

       # loop through column labels and add related data values to data dictionary
for i, col in enumerate(header):
           # create an indexed dictionary of all features
          features_by_index[i] = col
          x = []
          for line in lines[1:]:
              x.append(float(line.split(";")[i]))
          _data[col] = x

   # Create list of winequality Point objects
original_data_points = []
for i in range(len(_data["fixed acidity"])):
      features = [_data[col][i] for col in features_by_index.values()]
      P = Point(features, label=_data["quality"][i], name=str(i+1))
      original_data_points.append(P)
      
      
# At this point, data is all lumped together in one list/column. 
# I need to separate into individual rows under column headers, and run each row through instance of Point class.

# Illustrating comment above.

lines[:5]

_data_ = []

for d_line in lines:

    # separating the words (strings) by comma ","
    _line_ = d_line.split(",")

    # reading the data and turning them into their correct types
    _line_f = [float(e) for e in _line_[:-1]]
    _line_f.append(_line_[-1][:-1])

    # putting the data together
    _data_.append(_line_f)
    
_data_[:4] # testing

"""After this cell, the data is structured correctly, so that each row is its own list. Now it can be run through the Point class."""

_data_copy = _data_[:-1]

original_data_points = []

for i, _d_ in enumerate(_data_[:-1]): 
    _p_ = Point(_d_[:-1], label = _d_[-1], name=str(i+1))
    #original_data_points.append(_p_)
    
#print(_p_.get_features())

# Dividing into training and test set using general-case function... Although some adaptations were made, I used another method for the ultimate prediction.

def divide_dataset(_data_, training_fraction = 0.8, seed = random.randint(1,100000)):
  _len_ = len(_data_)+1
  _number_of_data_in_training_ = int(round(training_fraction*_len_))
  
  random.seed(seed)
  sample_indices = random.sample(range(len(_data_)), _number_of_data_in_training_)

  test_dataset = []
  training_dataset = []
  
  for i, point in enumerate(_data_):
          if i in sample_indices:
              training_dataset.append(point)

          else:
              test_dataset.append(point)

  return training_dataset, test_dataset

# Making it reproducible with setting seed

data_points, test_dataset = divide_dataset(original_data_points, training_fraction=0.8, seed = 1000)

wine_plot = plt.figure(figsize=(8,8))

x, y = [], []

for point in data_points:
    x.append(point.get_features()[0]),
    y.append(point.get_features()[1]),

_x_, _y_ = [[], [], []], [[], [], []]

x = np.array(x)
y = np.array(y)
_labels_=np.array(_labels_)


for i, _label_ in enumerate(unique_labels):
    _x_[i] = x[_labels_==_label_]
    _y_[i] = y[_labels_==_label_]

plt.plot(_x_[0], _y_[0], ".", markersize=14, label=unique_labels[0])
plt.plot(_x_[1], _y_[1], ".", markersize=14, label=unique_labels[1])
plt.plot(_x_[2], _y_[2], ".", markersize=14, label=unique_labels[2])


plt.xlabel(features_by_index[0], fontsize = 22)
plt.ylabel(features_by_index[1], fontsize = 22)

plt.legend(fontsize=15)

plt.show()