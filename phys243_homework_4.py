# -*- coding: utf-8 -*-
"""PHYS243 Homework 4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15niAW_ap5bW713eU42TluucHu51ma5T_

For this assignment, I have pulled in relevant codes from the notebooks referenced in class as well as an outside tutorial that I used as a reference. Citations  are provided when outside references were used. To reiterate discussed criteria, I have focused on explaining the concepts associated with running the different algorithms (KNN, decision trees, random forest) and added relevant code examples adapted as best I could to the MNIST dataset. 

The code output is not complete or functional, but the theoretical understanding of the methodology, steps, and reasoning is displayed.

I am aware that the first page of homework deals with binary classifiers and the second pertains to multi-class classifiers. The code and step-by-step methodology of each method (KNN, decision tree, and random forest) are listed in that order while an analysis between the two available classifications is addressed last in this notebook.

The initial few cells (without text) are pulled in from the notebooks as generic settings for plots, libaries, modules, and parameter settings. This has been standard in all projects and the plot configurations, libraries, and formatting is self-explanatory. I will include text boxes for further explanation of techniques and methodology throughout.
"""

import numpy as np
import random
import matplotlib.pyplot as plt

# font parameters dictionary
font = {'family' : 'serif',
        'weight' : 'bold',
        'size'   : 18,}


# figure parameters dictionary
figure = {"figsize" : (6,6),
          "dpi" : 120,
          "facecolor" : "w",
          "edgecolor" : "k",}

# use LaTeX fonts in the plot
plt.rc('text', usetex=False)

# ticks settings
plt.rc('xtick',labelsize=10)
plt.rc('ytick',labelsize=10)

# axes setting
plt.rc('axes', titlesize=22, labelsize=18)     # fontsize of the axes title, labels

# Set the font parameters
plt.rc('font', **font)

# Set the figure parameters
plt.rc("figure", **figure)

"""Importing the MNIST data and showing the arrays..."""

try:
    from sklearn.datasets import fetch_openml
    mnist = fetch_openml('mnist_784', version=1, cache=True)
    mnist.target = mnist.target.astype(np.int8)
except ImportError:
    from sklearn.datasets import fetch_mldata
    mnist = fetch_mldata('MNIST original')
mnist["data"], mnist["target"]

"""As an example, I am plotting one of the digits (a six) as a checkpoint to ensure the data is available in a correct format and the plots give a satisfactory depiction."""

example = mnist["data"][-1].reshape(28, 28)

import matplotlib as mpl

plt.imshow(example, cmap=mpl.cm.binary)

"""After testing that data was loaded correctly, this following cell enables splitting of test/training data and imports the library necessary for establishing accuracy checkpoints along the way. 

The steps of KNN are as follows:

1. Collect data (MNIST)
2. Ensure that numeric values x, y will be quantifying the distance measures.
3. Calculate the distance (in my example, we are using Minkowski distance with p set to 1, 2, and 3).
4. There is no unsupervised training sequence, as we have the original dataset with labeled values.
     Find the error rate (using accuracy score library imported below from sklearn).
5. Run actual data through algorithm
"""

from sklearn.model_selection import train_test_split

from sklearn.metrics import accuracy_score

"""**Problem 1. KNN Analysis**

For this problem, I have set three definitions for the Minkowski distance to reflect values of p (1, 2, 3). The varying values of p provides some varying power in the Minkowski definition. We are measuring the distance between vectors x1 and x2. Increasing the mean power value, p, will expand the unit distance from center that the distance points may land. When p is 1, we have a Manhattan distance. When p is 2, we have the standard Euclidean distance. When p increases towards infinity, we will see a Chebyshev distance which can be represented exponentially as 1/p.  In this case, the distance between vectors x1 and x2 is maximum along any and all coordinate dimensions in vector space.

The** pros** of using KNN is that this method can be fairly accurate, is insensitive to outliers, and needs no assumptions to be made regarding parameters or distribution of the data. It is advisable for numeric or nominal type data.

The **cons** of using KNN is that it requires significant amount of memory, hence the limit to 5000 values in the following analysis.
"""

X_train, X_test, y_train, y_test = train_test_split(mnist["data"][:5000], mnist["target"][:5000], test_size=0.2)

"""We will only be using 5000 samples due to computing space parameters."""

train_images = np.asarray(X_train[:5000])
train_labels = np.asarray(y_train[:5000])
test_images = np.asarray(X_test)
test_labels = np.asarray(y_test)

# Minkowski distance definition, p = 1, MANHATTAN

def Minkowski_d1(x1, x2, p=1):
    
    _sum_=0
    
    for x1_, x2_ in zip(x1, x2):
        dummy = pow(abs(x1_-x2_), p)
        _sum_+=dummy
    distance = pow(_sum_, 1/p)
    return distance

# Minkowski distance definition, p = 2, EUCLIDEAN

def Minkowski_d2(x1, x2, p=2):
    
    _sum_=0
    
    for x1_, x2_ in zip(x1, x2):
        dummy = pow(abs(x1_-x2_), p)
        _sum_+=dummy
    distance = pow(_sum_, 1/p)
    return distance

# Minkowski distance definition, p = 3

def Minkowski_d3(x1, x2, p=3):
    
    _sum_=0
    
    for x1_, x2_ in zip(x1, x2):
        dummy = pow(abs(x1_-x2_), p)
        _sum_+=dummy
    distance = pow(_sum_, 1/p)
    return distance

"""The following cell predicts the new data value's classification label be evaluating the surrounding data's labels. We are using a Minkowski distance formula which allows the distances to be sorted according the the power of p. This cell would be repeated with the other powers of p (2, 3) and is shown with a power of p = 1 which is Manhattan.

Labels of nearest neighors (within k closest values) are extracted and read. After evaluating the labels of surrounding data points, the code using "return find_majority" identifies the label that had the most "votes" from the k nearest neighbors and will select this classification.

Although at this point the classification has been selected, we must check the accuracy of the assignment. The code that includes the for loop referencing the training images and counting the correct predictions is the accuracy checkpoint.  Unfortunately, errors from this block of code arose and necessitate my explanations of the conceptual processes of each method of classification.
"""

def predict(k, train_images, train_labels, test_images):

    distance = [(Minkowski_d(test_image, image), label)

                    for (image, label) in zip(train_images, train_labels)]

    # sort the distances list by distances
    by_distances = sorted(distance, key=lambda (distance, _): distance)

    # extract only k closest labels
    k_labels = [label for (_, label) in by_distances[:k]]

    # return the majority voted label
    return find_majority(k_labels)


# Predicting and printing the accuracy

i = 0
total_correct = 0

for test_image in test_images:

    pred = predict(10, train_images, train_labels, test_image)

    if pred == test_labels[i]:

        total_correct += 1

    acc = (total_correct / (i+1)) * 100

    print('test image['+str(i)+']', '\tpred:', pred, '\torig:', test_labels[i], '\tacc:', str(round(acc, 2))+'%')

    i += 1

"""**Problem 2. Setting up decision tree analysis and establishing performance check to run at every point**

For problem 2, we will be walking through the process of setting up decision tree analysis. In decision tree analysis, each leaf represents a test. The direction runs root to leaf, and each step in that process is a test that yields a classification. 

There are three kinds of nodes: a decision node, a chance node, and an end node which is the result. In order to complete a decision tree, the dataset must be split, we must establish features on which to splits the data, and we must decide the value of the feature that forces the split.

Additionally, we need to be able to stop splitting. It would be meaningless to continue the test process until each point has a node. We should stop when node has just one classification option for objects as this allows some sorting to still take place. If we continue the algorithm too long, the decision tree will "overfit" the data and information will decrease into meaninglessness. 

The goal in decision tree analysis is to generate information from the dataset, but there is a relationship between information generation and depth of analysis. As depth of analysis increases, error decreases but is then prone to overfitting after a threshhold has been reached. When overfitting occurs, our analysis has stopped finding correlation and relationship between data classes and is just reporting "noise".

To conduct decision tree analysis, we must do the following steps:

1) Make the first decision.
2) Determine potential subsets based on the first decision in point 1.
3) Continue subsetting based on decisions until branches have the same class. At this point, we need to stop splitting.

In order to make meaningful splits, we need to measure entropy. This measurement needs to occur at every feature decision to determine if we chose a meaningful feature on which to split. Entropy describes the measurement of information. Comparing measurements of entropy for different feature options enables the most meaningful feature to be determined in order to make the most meaningful split. 


**The information for a value in a list, Xi, is defined as:**

l(Xi) = log2*p(Xi)

where p(Xi) is the probability of choosing a class. We ideally need to know all expected values, l(Xi) of all information of all values of a class (denoted by i).

**Entropy, being the measurement of information, is defined as:**

the sum of p(Xi)*l(Xi) which gives rise to the sum of p(Xi)logb(p(Xi))

**We take the log because the information accumulated in entropy is additive.**
"""

# Entropy

def Entropy(prob_X):
    """
  Definition from the following:

    Shanon and Weaver, 1949

    -> Links to paper :
    --> http://math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf
    --> https://ieeexplore.ieee.org/document/6773024


              Entropy = Σ_i p_i * log2 (p_i)


            prob_X (a list/array of variables):  

    """
    import math
    _sum_ = 0

    _tot_ = 0
    # checks
    for prob in prob_X:
        assert prob >= 0, "Negative probability is not accepted!!!"
        _tot_ += prob

#     if _tot_!=1:
#         print("Inputs are not normalized added up to {}, will be normalized!!".format(_tot_))

    for prob in prob_X:
        if _tot_==0:
            continue

        prob = prob/_tot_
        if prob == 0:
            pass
        else:
            _sum_ += prob * math.log2(prob)

    return abs(_sum_)


def Boolean_Entropy(q):
    
    assert q >= 0 and q <= 1, "q = {} is not between [0,1]!".format(q)

    return Entropy([q, 1-q])


def Boolean_Entropy_counts(p, n):
   
    if n==0 and p==0:
        return 0
    q = p / (n + p)
    return Boolean_Entropy(q)

"""Now that we have defined entropy in the code, the cells below measure the entropy gain from a features (as shown in counting distinct outcomes).

We are able to see "_sum_" which is the result of evaluating the number of positive and negative counts from the entropy measurements. 

Since entropy is the amount of disorder (information) within data, we want to make sure that entropy does not equal 0 as this would be indicative of no information being passed along from recent split. 0 entropy is 0 information and indicates that we have split in too great of a depth.
"""

# Entropy gain

def Remainder_Entropy(Attr, outcome):

    set_of_distinct_values = set(Attr)

    count_distict_values = len(set_of_distinct_values)
    count_distict_outcomes = len(set(outcome))

    assert count_distict_outcomes <= 2, "{} different outcomes but expected Boolean"


    count_total_positives = len([i for i in outcome if i!=0])
    count_total_negatives = len(outcome) - count_total_positives

    import numpy as np

    Attr_np = np.array(Attr)
    outcome_np = np.array(outcome)

    _sum_ = 0

    for value in set_of_distinct_values:
        _outcome_ = outcome_np[Attr_np==value]
        count_positives = len([i for i in _outcome_ if i!=0])
        count_negatives = len(_outcome_) - count_positives

        _entropy_ = Boolean_Entropy_counts(count_positives, count_negatives)
        _weights_ = (count_positives + count_negatives)
        _weights_ = _weights_ / (count_total_positives + count_total_negatives)

        _sum_ += _weights_ * _entropy_

    return _sum_

"""Similarly to defining entropy gain, information gain is important as we desire information from the algorithm. As frequency of cases increases, information decreases, but **we must evaluate information gain after every split.**"""

# Information gain

def Information_Gain(Attr, outcome):
    count_total_positives = len([i for i in outcome if i!=0])
    count_total_negatives = len(outcome) - count_total_positives

    initial_entropy = Boolean_Entropy_counts(count_total_positives, count_total_negatives)
    remaining_entropy = Remainder_Entropy(Attr, outcome)

    info_gain = initial_entropy - remaining_entropy

    return info_gain

# Defining min and max

identity = lambda x: x

argmin = min
argmax = max


def argmin_random_tie(seq, key=identity):
    """Return a minimum element of seq; break ties at random."""
    return argmin(shuffled(seq), key=key)


def argmax_random_tie(seq, key=identity):
    """Return an element with highest fn(seq[i]) score; break ties at random."""
    return argmax(shuffled(seq), key=key)


def shuffled(iterable):
    """Randomly shuffle a copy of iterable."""
    items = list(iterable)
    random.shuffle(items)
    return items

def check_equal(iterator):
    iterator = iter(iterator)
    try:
        first = next(iterator)
    except StopIteration:
        return True
    return all(first == rest for rest in iterator)

def probability(p):
    """Return true with probability p."""
    return p > random.uniform(0.0, 1.0)

"""Initializing the class dataset and attributes (features)."""

class Data_Set:

  def __init__(self, examples=None, attributes=None,  attribute_names=None,
                 target_attribute = -1,  input_attributes=None,  values=None,
                 distance_measure = mean_boolean_error, name='',  source='',
                 excluded_attributes=(), file_info=None):
self.file_info = file_info
        self.name = name
        self.source = source
        self.values = values
        self.distance = distance_measure
        self.check_values_flag = bool(values)

# Initialize examples from a list
        if examples is not None:
            self.examples = examples
        elif file_info is None:
            raise ValueError("No Examples! and No Address!")
        else:
            self.examples = _read_data_set(file_info[0], file_info[1], file_info[2])

        # Attributes are the index of examples. can be overwrite
        if self.examples is not None and attributes is None:
            attributes = list(range(len(self.examples[0])))

        self.attributes = attributes

        # Initialize attribute_names from string, list, or to default
        if isinstance(attribute_names, str):
            self.attribute_names = attribute_names.split()
        else:
            self.attribute_names = attribute_names or attributes

        # set the definitions needed for the problem
        self.set_problem(target_attribute, input_attributes=input_attributes,
                         excluded_attributes=excluded_attributes)


    def get_attribute_num(self, attribute):
        if isinstance(attribute, str):
            return self.attribute_names.index(attribute)
        else:
            return attribute

    def set_problem(self, target_attribute, input_attributes=None, excluded_attributes=()):
        """
        By doing this we set the target, inputs and excluded attributes.

        This way, one DataSet can be used multiple ways. inputs, if specified,
        is a list of attributes, or specify exclude as a list of attributes
        to not use in inputs. Attributes can be -n .. n, or an attrname.
        Also computes the list of possible values, if that wasn't done yet."""

        self.target_attribute = self.get_attribute_num(target_attribute)

        exclude = [self.get_attribute_num(excluded) for excluded in excluded_attributes]

        if input_attributes:
            self.input_attributes = remove_all(self.target_attribute, input_attributes)
        else:
            inputs = []
            for a in self.attributes:
                if a != self.target_attribute and a not in exclude:
                    inputs.append(a)
            self.input_attributes = inputs

        if not self.values:
            self.update_values()
        self.sanity_check()


    def sanity_check(self):
        """Sanity check on the fields."""

        assert len(self.attribute_names) == len(self.attributes)
        assert self.target_attribute in self.attributes
        assert self.target_attribute not in self.input_attributes
        assert set(self.input_attributes).issubset(set(self.attributes))
        if self.check_values_flag:
            # only check if values are provided while initializing DataSet
            [self.check_example(example) for example in self.examples]


    def check_example(self, example):
        if self.values:
            for attr in self.attributes:
                if example[attr] not in self.values:
                    raise ValueError("Not recognized value of {} for attribute {} in {}"
                                     .format(example[attr], attr, example))


    def add_example(self, example):
        self.check_example(example)
        self.examples.append(example)


    def update_values(self):
        self.values = list(map(unique, zip(*self.examples)))


    def remove_examples(self, value=""):
        self.examples = [example for example in examples if value not in example]

    def sanitize(self, example):  
        """Copy of the examples with non input_attributes replaced by None"""

"""The following lengthy cell initializes the attributes and establishes locations for examples."""

def __init__(self, examples=None, attributes=None,  attribute_names=None,
                 target_attribute = -1,  input_attributes=None,  values=None,
                 distance_measure = mean_boolean_error, name='',  source='',
                 excluded_attributes=(), file_info=None):

        """
        Accepts any of DataSet's fields. Examples can also be a
        string or file from which to parse examples using parse_csv.
        Optional parameter: exclude, as documented in .setproblem().

        >>> DataSet(examples='1, 2, 3')
        <DataSet(): 1 examples, 3 attributes>
        """

        self.file_info = file_info
        self.name = name
        self.source = source
        self.values = values
        self.distance = distance_measure
        self.check_values_flag = bool(values)

        # Initialize examples from a list
        if examples is not None:
            self.examples = examples
        elif file_info is None:
            raise ValueError("No Examples! and No Address!")
        else:
            self.examples = _read_data_set(file_info[0], file_info[1], file_info[2])

        # Attributes are the index of examples. can be overwrite
        if self.examples is not None and attributes is None:
            attributes = list(range(len(self.examples[0])))

        self.attributes = attributes

        # Initialize attribute_names from string, list, or to default
        if isinstance(attribute_names, str):
            self.attribute_names = attribute_names.split()
        else:
            self.attribute_names = attribute_names or attributes

        # set the definitions needed for the problem
        self.set_problem(target_attribute, input_attributes=input_attributes,
                         excluded_attributes=excluded_attributes)



    def get_attribute_num(self, attribute):
        if isinstance(attribute, str):
            return self.attribute_names.index(attribute)
        else:
            return attribute

    def set_problem(self, target_attribute, input_attributes=None, excluded_attributes=()):
        """
        By doing this we set the target, inputs and excluded attributes.

        This way, one DataSet can be used multiple ways. inputs, if specified,
        is a list of attributes, or specify exclude as a list of attributes
        to not use in inputs. Attributes can be -n .. n, or an attrname.
        Also computes the list of possible values, if that wasn't done yet."""

        self.target_attribute = self.get_attribute_num(target_attribute)

        exclude = [self.get_attribute_num(excluded) for excluded in excluded_attributes]

        if input_attributes:
            self.input_attributes = remove_all(self.target_attribute, input_attributes)
        else:
            inputs = []
            for a in self.attributes:
                if a != self.target_attribute and a not in exclude:
                    inputs.append(a)
            self.input_attributes = inputs

        if not self.values:
            self.update_values()
        self.sanity_check()


    def sanity_check(self):
        """Sanity check on the fields."""

        assert len(self.attribute_names) == len(self.attributes)
        assert self.target_attribute in self.attributes
        assert self.target_attribute not in self.input_attributes
        assert set(self.input_attributes).issubset(set(self.attributes))
        if self.check_values_flag:
            # only check if values are provided while initializing DataSet
            [self.check_example(example) for example in self.examples]


    def check_example(self, example):
        if self.values:
            for attr in self.attributes:
                if example[attr] not in self.values:
                    raise ValueError("Not recognized value of {} for attribute {} in {}"
                                     .format(example[attr], attr, example))


    def add_example(self, example):
        self.check_example(example)
        self.examples.append(example)


    def update_values(self):
        self.values = list(map(unique, zip(*self.examples)))


    def remove_examples(self, value=""):
        self.examples = [example for example in examples if value not in example]

    def sanitize(self, example):  
        """Copy of the examples with non input_attributes replaced by None"""
        _list_ = []
        for i, attr_i in enumerate(example):
            if i in self.input_attributes:
                _list_.append(attr_i)
            else:
                _list_.append(None)
        return _list_

    def train_test_split(self, test_fraction=0.3, Seed = 99):
        import numpy as np

        examples = self.examples
        atrrs = self.attributes
        atrrs_name = self.attribute_names
        target = self.target_attribute
        input_ = self.input_attributes
        name = self.name

        np.random.seed(Seed)
        _test_index = np.random.choice(list(range(len(examples))), int(test_fraction * len(examples)), replace=False)

        test_examples = [example for i, example in enumerate(examples) if i in _test_index]

        train_examples = [example for example in examples if example not in test_examples]

        # Separate train and test dataset. Branch and leaf algorithms will follow.
        
        Test_data_set = Data_Set(examples=test_examples,
                                 attributes=atrrs,
                                 attribute_names=attr_names,
                                 target_attribute=target,
                                 input_attributes=input_,
                                 name=name + " Test set",)

        Train_data_set = Data_Set(examples=train_examples,
                                 attributes=atrrs,
                                 attribute_names=attr_names,
                                 target_attribute=target,
                                 input_attributes=input_,
                                 name=name + " Train set",)

        return Train_data_set, Test_data_set


    def __repr__(self):
        return '<DataSet({}): with {} examples, and {} attributes>'.format(
            self.name, len(self.examples), len(self.attributes))

"""Defining the class of the decision tree branch tests the attribute being evaluated. The branch comes before the leaf/node, which is the end result classification. 

Within the Decision_Branch classification, we must initialize the attibutes and identify attributes that we are interested in that will be evaluated by node/leaf tests. After testing the node, the algorithm classifies the datapoint using the result of the test in the previously mentioned branch decision.

At this point, the branch may be added when the attribute is identified and the following remaining steps within the cell reflect adding lines and subsetting within the branch's domain to form the tree. 

At this stage, the average methodology necessitated in Random Forests has not yet taken place.
"""

class Decision_Branch:
    """
    A branch of a decision tree holds an attribute to test, and a dict
    of branches for each attribute's values.
    """

    def __init__(self, attribute, attribute_name=None, default_child=None, branches=None):

        self.attribute = attribute
        self.attribute_name = attribute_name or attribute
        self.default_child = default_child
        self.branches = branches or {}

    def __call__(self, example):
        attribute_val = example[self.attribute]
        if attribute_val in self.branches:
            return self.branches[attribute_val](example)
        else:
            # return default class when attribute is unknown
            return self.default_child(example)

    def add(self, value, subtree):
        self.branches[value] = subtree

    def display_out(self, indent=0):
        name = self.attribute_name
        print("Test", name)
        for value, subtree in self.branches.items():
            print(" " * indent * 5, name, '=', value, "--->", end=" ")
            subtree.display_out(indent + 1)
        # New line
        print()

    def __repr__(self):
        return ('Decision_Branch({}, {}, {})'
                .format(self.attribute, self.attribute_name, self.branches))

"""After setting up the algorithm to evaluate/generate branches, the next step in the journey from root to leaf is the end result (the leaf). Defining the leaf class allows the class to hold the final result that has been determined in the previous steps. The leaf does not perform any more testing. It represents a location that holds the result of the previous rounds of testing and subsetting. The leaf holds the answer to our classification and denotes the depth of the analysis desired that holds the answer to our research question.

In the context of the MNIST dataset, the leafs' results will have labeled the datapoints with a value between 0-9 (our attribute labels).
"""

class Decision_Leaf:
   
    def __init__(self, result):
        self.result = result

    def __call__(self, example):
        return self.result

    def display_out(self, indent=0):
        print('RESULT =', self.result)

    def __repr__(self):
        return repr(self.result)

"""Now that the definitions of branchs and leaves have been established, we can run the dataset through the decision tree algorithm. This step of the process involves defining the overarching algorithm that runs actual data through the previously defined classes for test and the ultimate classification decisions can be displayed at this point. 

We establish the target, values attributes, and values themselves. When plurality is defined, the most frequently observed target value is returned. Plurality assumes that the target is mostly nonbinary, as binary targets would yield a "majority" (but the same concept).
"""

def Decision_Tree_Learner(dataset):
    

    target, values = dataset.target_attribute, dataset.values

    def decision_tree_learning(examples, attrs, parent_examples=()):
        if not examples:
            return plurality(parent_examples)
        elif same_class_for_all(examples):
            return Decision_Leaf(examples[0][target])
        elif not attrs:
            return plurality(examples)
        else:
            A = choose_important_attribute(attrs, examples)
            tree = Decision_Branch(A, dataset.attribute_names[A], plurality(examples))

            for (vk, exs) in split_by(A, examples):
                subtree = decision_tree_learning(
                    exs, remove_all(A, attrs), examples)
                tree.add(vk, subtree)
            return tree

    def plurality(examples):
        """Return the most occured target value for this set of examples.
        (If binary target this is the majority, otherwise plurality)"""
        most_occured = argmax_random_tie(values[target],
                                    key=lambda v: count_example_same_attr(target, v, examples))
        return Decision_Leaf(most_occured)

    def count_example_same_attr(attr, val, examples):
        """Count the number of examples that have example[attr] = val."""
        return sum(e[attr] == val for e in examples)

    def same_class_for_all(examples):
        """Are all these examples in the same target class?"""
        _class_ = examples[0][target]
        return all(example[target] == _class_ for example in examples)

    def choose_important_attribute(attrs, examples):
        """Choose the attribute with the highest information gain."""
        return argmax_random_tie(attrs,
                                 key=lambda a: information_gain(a, examples))

    def information_gain(attr, examples):
        """Return the expected reduction in entropy from splitting by attr."""
        def _entropy_(examples):
            count = []
            for val in values[target]:
                count.append(count_example_same_attr(target, val, examples))
            return Entropy(count)

        N = len(examples)
        remainder = sum((len(examples_i)/N) * _entropy_(examples_i)
                        for (v, examples_i) in split_by(attr, examples))
        return _entropy_(examples) - remainder

    def split_by(attr, examples):
        """Return a list of (val, examples) pairs for each val of attr."""
        return [(v, [e for e in examples if e[attr] == v])
                for v in values[attr]]

    return decision_tree_learning(dataset.examples, dataset.input_attributes)

"""**Problem 3. Random Forest**

Random forest analysis is an extension of the decision tree described above. In information theory, we look for lower probability events since they carry more information or insight into the data than higher probability events with higher confidence in the outcome. Also, we evaluate entropy, as described previously. 

A random forest generates a series of decision trees that are found from boostrapped samples (bagged). Bagging refers to sampling with replacement. 

The final result of a random forest is an average classification of decision trees.

An advantage of generating a random forest over decision tree analysis is that the random forest, since it is fueled by averages, avoids overfitting. There is no reasonable possibility that each leaf is a single option classification, which is the disadvantageous possibility of the result of decision tree analysis.

In the context of the MNIST dataset, random forest analysis should
"""

def Random_Forest(dataset, n=5, verbose=False):
    
    def data_bagging(dataset, m=0):
        """Sample m examples with replacement"""
        n = len(dataset.examples)
        return weighted_sample_with_replacement(m or n, dataset.examples, [1]*n)

    def feature_bagging(dataset, p=0.7):
        """Feature bagging with probability p to retain an attribute"""
        inputs = [i for i in dataset.input_attributes if probability(p)]
        return inputs or dataset.input_attributes

    def predict(example):
        if verbose:
            print([predictor(example) for predictor in predictors])
        return mode(predictor(example) for predictor in predictors)

    predictors = [Decision_Tree_Learner(Data_Set(examples=data_bagging(dataset),
                                                 attributes=dataset.attributes,
                                                 attribute_names=dataset.attribute_names,
                                                 target_attribute=dataset.target_attribute,
                                                 input_attributes=feature_bagging(dataset))) for _ in range(n)]

    return predict

address = "mnist"
attribute_names = ["0", "1", "2", "3", "4", "5", "6", "7", "8", "9"]
Digit_Data = Data_Set(name = "MNIST", target_attribute=10,
                     file_info=(address, 0, ","), attribute_names=attribute_names)

TREE = Decision_Tree_Learner(Digit_Data)

TREE.display_out()

f = Random_Forest(Digit_Data, n = 5000)
Digit_Data_cp = Digit_Data

def _find_the_class_(e, classes):
    number_of_classes = len(classes)
    for i in range(number_of_classes):
        try:
            if e <= classes[i+1] and e>= classes[i]:
                e = (classes[i]+classes[i+1])/2
        except IndexError:
            pass
    return e


Digit_Data_cp = Digit_Data

m = 3
indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]


for e in Digit_Data_cp.examples:
    for index in indices:
        min_v = min(Digit_Data_cp.values[index])
        max_v = max(Digit_Data_cp.values[index])

        splits = [(min_v + (max_v - min_v) * i / m) for i in range(m+1)]
        e[index] = _find_the_class_(e[index], splits)

Digit_Data_cp.update_values()

_new_tree_ = Decision_Tree_Learner(Digit_Data_cp)

"""**Problem 4. K-Means algorithm to identify 10 clusters for unsupervised learning.**

Scaling and resplitting data. I was struggling with the format of the imported data from the previous exercises and used a tutorial listed below for help loading the handwritten digits dataset from UCI. The methodology is relevent in either dataset scenario.
"""

from sklearn import datasets

digits = datasets.load_digits()

print(digits)

from sklearn.preprocessing import scale
from sklearn import cluster
from sklearn import metrics
from sklearn.model_selection import train_test_split

data = scale(digits.data)

X_train, X_test, y_train, y_test, images_train, images_test = train_test_split(data, digits.target, digits.images, test_size=0.25, random_state=42)

#Splitting the data (previous cell) and finding number of features

n_samples, n_features = X_train.shape

# labels for training

n_digits = len(np.unique(y_train))

# Number of cluster must be 10 as there will be 10 digits

clf = cluster.KMeans(init='k-means++', n_clusters=10, random_state=42)

# can see how many centroid options the model tests out (_init)
# Fit data for model
clf.fit(X_train)

# plot taken from datacamp tutorial "Scikit-Learn Tutorial", Feb 25th, 2019. Not part of requirements, but a checkpoint.

fig = plt.figure(figsize=(8, 3))

fig.suptitle('Cluster Center Images', fontsize=14, fontweight='bold')

# For all labels (0-9)
for i in range(10):
    
    ax = fig.add_subplot(2, 5, 1 + i)
    
    ax.imshow(clf.cluster_centers_[i].reshape((8, 8)), cmap=plt.cm.binary)
    plt.show()

"""Predicting labels..."""

y_pred=clf.predict(X_test)

# Just using first 100 instances
print(y_pred[:100])

print(y_test[:100])

# See shape
clf.cluster_centers_.shape

# Testing model via confusion matrix

print(metrics.confusion_matrix(y_test, y_pred))

# Not great at all. Very low prediction accuracy

# Evaluating cluster quality metrics (borrowed from tutorial). Euclidean distance. I am only looking at what came to mind outside the tutorial (homogeneity, completeness, silhouette)

from sklearn.metrics import homogeneity_score, completeness_score, v_measure_score, adjusted_rand_score, adjusted_mutual_info_score, silhouette_score
print('% 9s' % 'inertia    homo   compl  v-meas     ARI AMI  silhouette')
print('%i   %.3f   %.3f   %.3f   %.3f   %.3f    %.3f'
#           %(clf.inertia_,
      homogeneity_score(y_test, y_pred),
      completeness_score(y_test, y_pred),
      v_measure_score(y_test, y_pred),
      adjusted_rand_score(y_test, y_pred),
      adjusted_mutual_info_score(y_test, y_pred),
      silhouette_score(X_test, y_pred, metric='euclidean')))

"""Clearly this model is not a good fit. I wanted to evaluate homogeneity in this context, since it should tell me how often clusters contain points of a single class (69%, not great).

Completeness score tells me how frequently points that are members of a given class are also elements of the same cluster (73% fairly often, also not great).

Silhouette score should be high in a well done model, since it would indicate that the cluster configuration is good/accurate. It is close to 0, so also poor quality.

**Problem 9. Comment on any signiﬁcant diﬀerence between your results for the binary classiﬁer vs multi-class classiﬁers.**

Although my actual code is not complete or free of bugs enough to generate complete analyses in many of the problems thus far, I would expect that binary classifiers have a few main differences from multi-class classifiers. There would be a possibility that binary classification might be cumbersome when dealing with many different classes. This would not appear to be an issue in our dataset with 10 digit labels, however. Speed would be an advantage of the binary classifiers, since they should be able to complete their boolean test fasters than a multiple class decision tree. It would follow that perhaps the subsetting steps in the decision tree/random forest analyses might be more efficient.

Multi-class classifiers might hold an advantage when working with datasets with a more complicated system of labels/classes. Although this seems intuitive, it would be an important point of evaluation when deciding which algorithm style to pursue.
"""

