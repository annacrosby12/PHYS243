# -*- coding: utf-8 -*-
"""PHYS243_Homework_5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sAK2sp-ASVVVTIiuXxppmoiDedsgQuXe

**Importing data and converting categorical attributes to numeric data.**
"""

import pandas as pd
import numpy as np

# Define the headers since the data does not have any
headers = ["age", "workclass", "fnlwgt", "education", "education num", "marital-status", "occupation", "relationship", "race", "sex", "capital-gain", "capital-loss", "hours-per-week", "native-country", "income"]

# Reading in the data from the website (alternative that was most effective for me) and replacing blanks with NAn
df = pd.read_csv("http://mlr.cs.umass.edu/ml/machine-learning-databases/adult/adult.data",
                  header=None, names=headers, na_values="?" )
df.head()

"""Data import and initial structuring with headers is correct. Categorical data needs to be turned into numeric data. I will be using the One Hot Encoding method. Additional columns will be added due to this choice in encoding. I will be creating dummy variables in each attribute that is not continuous (is categorical)."""

# To understand nuances of One Hot Encoding, I referenced (but did not copy) the processes from https://pbpython.com/categorical-encoding.html

# Attributes that will be impacted by One Hot Encoding will be workclass, education, marital-status, occupation, relationship, race, sex, native-country, and income.
# The dataset dimensions will increase significantly due to the categorical encoding.

df2 = pd.get_dummies(df, columns=["workclass", "education", "marital-status", "occupation", "relationship", "race", "sex", "native-country", "income"])

df2.head()

"""As expected, the columns were significantly increased. I now have 110 columns when I originally had 15.

**Scaling Data.**
"""

# I will be scaling continuous data by z-scores. Attributes affected by scaling will be age, education num, capital-gain, capital-loss, and hours-per-week.

from sklearn import preprocessing
from pandas import Series, DataFrame
import pandas as pd
import numpy as np
import os
import matplotlib.pylab as plt
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report
import sklearn.metrics

from sklearn import datasets
from sklearn.ensemble import ExtraTreesClassifier



std_scale = preprocessing.StandardScaler().fit(df2[['age', 'education num', 'capital-gain', 'capital-loss', 'hours-per-week']])
df_std = std_scale.transform(df2[['age', 'education num', 'capital-gain', 'capital-loss', 'hours-per-week']])

# Checking to ensure that scaling has occurred by finding mean and std of age column...

df_std[1].mean() # Nearly 0

df_std[1].std() # Nearly 1

# identifying for predictors

list(df2.columns.values)

# C+P'd from output above

predictors = df2[['age',
 'fnlwgt',
 'education num',
 'capital-gain',
 'capital-loss',
 'hours-per-week',
 'workclass_ ?',
 'workclass_ Federal-gov',
 'workclass_ Local-gov',
 'workclass_ Never-worked',
 'workclass_ Private',
 'workclass_ Self-emp-inc',
 'workclass_ Self-emp-not-inc',
 'workclass_ State-gov',
 'workclass_ Without-pay',
 'education_ 10th',
 'education_ 11th',
 'education_ 12th',
 'education_ 1st-4th',
 'education_ 5th-6th',
 'education_ 7th-8th',
 'education_ 9th',
 'education_ Assoc-acdm',
 'education_ Assoc-voc',
 'education_ Bachelors',
 'education_ Doctorate',
 'education_ HS-grad',
 'education_ Masters',
 'education_ Preschool',
 'education_ Prof-school',
 'education_ Some-college',
 'marital-status_ Divorced',
 'marital-status_ Married-AF-spouse',
 'marital-status_ Married-civ-spouse',
 'marital-status_ Married-spouse-absent',
 'marital-status_ Never-married',
 'marital-status_ Separated',
 'marital-status_ Widowed',
 'occupation_ ?',
 'occupation_ Adm-clerical',
 'occupation_ Armed-Forces',
 'occupation_ Craft-repair',
 'occupation_ Exec-managerial',
 'occupation_ Farming-fishing',
 'occupation_ Handlers-cleaners',
 'occupation_ Machine-op-inspct',
 'occupation_ Other-service',
 'occupation_ Priv-house-serv',
 'occupation_ Prof-specialty',
 'occupation_ Protective-serv',
 'occupation_ Sales',
 'occupation_ Tech-support',
 'occupation_ Transport-moving',
 'relationship_ Husband',
 'relationship_ Not-in-family',
 'relationship_ Other-relative',
 'relationship_ Own-child',
 'relationship_ Unmarried',
 'relationship_ Wife',
 'race_ Amer-Indian-Eskimo',
 'race_ Asian-Pac-Islander',
 'race_ Black',
 'race_ Other',
 'race_ White',
 'sex_ Female',
 'sex_ Male',
 'native-country_ ?',
 'native-country_ Cambodia',
 'native-country_ Canada',
 'native-country_ China',
 'native-country_ Columbia',
 'native-country_ Cuba',
 'native-country_ Dominican-Republic',
 'native-country_ Ecuador',
 'native-country_ El-Salvador',
 'native-country_ England',
 'native-country_ France',
 'native-country_ Germany',
 'native-country_ Greece',
 'native-country_ Guatemala',
 'native-country_ Haiti',
 'native-country_ Holand-Netherlands',
 'native-country_ Honduras',
 'native-country_ Hong',
 'native-country_ Hungary',
 'native-country_ India',
 'native-country_ Iran',
 'native-country_ Ireland',
 'native-country_ Italy',
 'native-country_ Jamaica',
 'native-country_ Japan',
 'native-country_ Laos',
 'native-country_ Mexico',
 'native-country_ Nicaragua',
 'native-country_ Outlying-US(Guam-USVI-etc)',
 'native-country_ Peru',
 'native-country_ Philippines',
 'native-country_ Poland',
 'native-country_ Portugal',
 'native-country_ Puerto-Rico',
 'native-country_ Scotland',
 'native-country_ South',
 'native-country_ Taiwan',
 'native-country_ Thailand',
 'native-country_ Trinadad&Tobago',
 'native-country_ United-States',
 'native-country_ Vietnam',
 'native-country_ Yugoslavia']]

targets = df2['income_ >50K']

pred_train, pred_test, tar_train, tar_test  = train_test_split(predictors, targets, test_size=.4)

# shape/dimensions of the DataFrame
pred_train.shape
pred_test.shape
tar_train.shape
tar_test.shape

from sklearn.ensemble import RandomForestClassifier

# n_estimators is the amount of trees to build
classifier=RandomForestClassifier(n_estimators=25)
# fit the RandomForest Model
classifier=classifier.fit(pred_train,tar_train)
# prediction scoring of the model (array of binary 0-1)
predictions=classifier.predict(pred_test)

sklearn.metrics.confusion_matrix(tar_test,predictions)
sklearn.metrics.accuracy_score(tar_test, predictions)

"""Now Running SVM

The following code is NOT copied from the following link, but the information in the link was used as a reference point when I encountered errors. https://stackabuse.com/implementing-svm-and-kernel-svm-with-pythons-scikit-learn/
"""

from sklearn.svm import SVC
svclassifier = SVC(kernel='linear')
svclassifier.fit(pred_train, tar_train)

y_pred = svclassifier.predict(pred_test)

from sklearn.metrics import classification_report, confusion_matrix
print(confusion_matrix(tar_test,y_pred))
print(classification_report(tar_test,y_pred))

"""The confusion matrix relates important information regarding the accuracy of the model, as mentioned in the lectures. By evaluating the confusion matrix, we can see that the model allowed 9509 instances of true negatives (TN, meaning the model correctly predicted that the adults did not earn $50K).

There were 440 instances of false positives (FP), in which the model predicted that the person earned at least $50K when they earned below the threshold. This is a disappointing amount of FP.

There were 2186 instances of false negatives (FN) in which the model incorrectly predicted that the person earned below the threshold.

Lastly there were 890 instances of true negatives (TN) in which the model accurately predicted that the adult earned below the threshold. This was, overall, not an impressively accurate model. 

The classification report is interpretted as follows: Out of 9949 instances of adults being predicted as earning below the threshold, the model interpreted 9509 correctly, which yields the recall value of 0.96 (9509/9949) which is very accurate. Similarly, out of 3076 instances of adults being predicted as earning at or above the threshold, the model predicted 890 correctly which yields the recall value of 0.29 (890/3076) which is very inaccurate. The precision value should ideally be 1 since it would be the value of TN and TP aligning with the support values, but we can see that the model had higher accuracy predicting those earning below the threshold (0.81) than those below the threshold (0.67).

Overall accuracy from SVM was 0.8. When comparing with the initial random forest analysis, which reported an accuracy of 0.85, SVM would be considered slightly less accurate. It also took much longer to run than the random forest analysis.

For help in interpretting the SVM performance, I referenced the following link:
https://stackoverflow.com/questions/30746460/how-to-interpret-scikits-learn-confusion-matrix-and-classification-report
"""